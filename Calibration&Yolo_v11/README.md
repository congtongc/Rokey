# Calibration과 Yolo V11을 사용한 6축 협동 로봇 객체 인식 학습

eye-in-hand 구조로 로봇 팔에 연결된 RGB-D 카메라를 사용하여 카메라에 보이는 객체를 인식하도록 학습을 진행했다.

## Calibration

- 카메라에 체커보드를 보여주고 여러 각도에서 20장 이상의 사진을 촬영하여 어떠한 구도에서 보여지더라도 해당 위치에 있는 객체는 같은 좌표임을 학습시켰다.
- 아래 코드를 작성하여 체커보드의 코너의 개수를 코드에 지정하여 객체 위치를 인식하기 유용하도록 한다.
   checkerboard_size = (7, 10)  # 내부 코너 개수

- 추가로 위의 과정을 통해 카메라에 보이는 객체의 위치 좌표 변환 과정이 진행된다.
- 좌표 변환 과정은 (베이스 좌표 -> Gripper 좌표) --> (Gripper 좌표 -> Camera 좌표) --> (Camera 좌표 -> Target 좌표) --> (Target 좌표 -> Base 좌표) 의 순서로 좌표 변환을 통해 카메라에 인식된 물체의 위치 좌표를 반환한다.
- 반환한 좌표 파일은 T_gripper2camera.npy 라는 이름으로 저장된다.

## Yolo V11
- epoch는 50, batch size는 15로 공구 데이터 셋을 학습시켜 그리퍼가 학습된 공구를 인식하면 집을 수 있도록 학습을 진행했다.
- 검증 결과 높은 정확도로 공구를 잘 찾는 결과를 얻었다.
- 학습시키는 과정에서 카메라에 보이는 물체가 유사하게 보이면 해당 물체를 학습된 물체로 인식한다는 단점을 알게 되었다.
- 따라서 검증 과정 진행 시 주변 정리 또는 물체를 잘 보이도록 하단에 흰 종이를 깔아 객체 인식 확률을 높일 수 있도록 해야겠다.

< 검증 결과 >

- Hammer 검증

  ![hammer_0](https://github.com/user-attachments/assets/91f10c50-27f3-43ab-bd18-97163a05f809)

- 다른 공구들 동시 검증

  ![result_0](https://github.com/user-attachments/assets/678db2e8-5218-4c78-8529-8538b8bcff0c)
